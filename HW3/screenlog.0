[01;32melbertgong@nlpfinal[00m:[01;34m/mnt/trunk/cs287-s18/HW3[00m$ echo "trying with changd[Ked clip grad norm, and baseline reinforc[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K update baseline change"
trying with changed clip grad norm, and update baseline change
[01;32melbertgong@nlpfinal[00m:[01;34m/mnt/trunk/cs287-s18/HW3[00m$ echo "f[Kif this doesn't work, i'm gonna start rewinding changes (padding) to  see what fixes"
if this doesn't work, i'm gonna start rewinding changes (padding) to see what fixes
[01;32melbertgong@nlpfinal[00m:[01;34m/mnt/trunk/cs287-s18/HW3[00m$ python3 main.py -at [K[K[K[K[K
Getting datasets!
{'src': <torchtext.data.field.Field object at 0x7ff0f77f46d8>, 'trg': <torchtext.data.field.Field object at 0x7ff0f77f47b8>}
119076
{'trg': ['David', 'Gallo', ':', 'This', 'is', 'Bill', 'Lange', '.', 'I', "'m", 'Dave', 'Gallo', '.'], 'src': ['David', 'Gallo', ':', 'Das', 'ist', 'Bill', 'Lange', '.', 'Ich', 'bin', 'Dave', 'Gallo', '.']}
[('.', 113253), (',', 67237), ('ist', 24189), ('die', 23778), ('das', 17102), ('der', 15727), ('und', 15622), ('Sie', 15085), ('es', 13197), ('ich', 12946)]
Size of German vocab 13353
[('.', 113433), (',', 59512), ('the', 46029), ('to', 29177), ('a', 27548), ('of', 26794), ('I', 24887), ('is', 21775), ("'s", 20630), ('that', 19814)]
Size of English vocab 11560
2 3
Source size torch.Size([9, 32])
Target size torch.Size([14, 32])
REMINDER!!! Did you create ../../models/HW3?????
Traceback (most recent call last):
  File "main.py", line 148, in <module>
    loss, reinforce_loss = model.forward(x_de, x_en, attn_type)
  File "/mnt/trunk/cs287-s18/HW3/models.py", line 137, in forward
    self.baseline = Variable(0.95*self.baseline.data + 0.05*avg_reward)
  File "/usr/local/lib/python3.5/dist-packages/torch/tensor.py", line 395, in data
    raise RuntimeError('cannot call .data on a torch.Tensor: did you intend to use autograd.Variable?')
RuntimeError: cannot call .data on a torch.Tensor: did you intend to use autograd.Variable?
[01;32melbertgong@nlpfinal[00m:[01;34m/mnt/trunk/cs287-s18/HW3[00m$ python3 main.py
Getting datasets!
{'src': <torchtext.data.field.Field object at 0x7fa11a708710>, 'trg': <torchtext.data.field.Field object at 0x7fa11a708630>}
119076
{'src': ['David', 'Gallo', ':', 'Das', 'ist', 'Bill', 'Lange', '.', 'Ich', 'bin', 'Dave', 'Gallo', '.'], 'trg': ['David', 'Gallo', ':', 'This', 'is', 'Bill', 'Lange', '.', 'I', "'m", 'Dave', 'Gallo', '.']}
[('.', 113253), (',', 67237), ('ist', 24189), ('die', 23778), ('das', 17102), ('der', 15727), ('und', 15622), ('Sie', 15085), ('es', 13197), ('ich', 12946)]
Size of German vocab 13353
[('.', 113433), (',', 59512), ('the', 46029), ('to', 29177), ('a', 27548), ('of', 26794), ('I', 24887), ('is', 21775), ("'s", 20630), ('that', 19814)]
Size of English vocab 11560
2 3
Source size torch.Size([18, 32])
Target size torch.Size([22, 32])
REMINDER!!! Did you create ../../models/HW3?????
Time 0m 21s, Epoch [1/3], Iter [100/3722], Loss: 7.3114, Reward: -6.67, Accuracy: 0.09, PPL: 1497.33
Time 0m 42s, Epoch [1/3], Iter [200/3722], Loss: 5.5803, Reward: -5.32, Accuracy: 0.09, PPL: 265.15
Time 1m 3s, Epoch [1/3], Iter [300/3722], Loss: 5.0035, Reward: -4.98, Accuracy: 0.09, PPL: 148.93
Time 1m 23s, Epoch [1/3], Iter [400/3722], Loss: 4.8631, Reward: -4.85, Accuracy: 0.09, PPL: 129.43
Time 1m 44s, Epoch [1/3], Iter [500/3722], Loss: 4.7152, Reward: -4.52, Accuracy: 0.09, PPL: 111.63
Time 2m 5s, Epoch [1/3], Iter [600/3722], Loss: 4.6963, Reward: -4.72, Accuracy: 0.10, PPL: 109.54
Time 2m 26s, Epoch [1/3], Iter [700/3722], Loss: 4.6219, Reward: -4.66, Accuracy: 0.10, PPL: 101.68
Time 2m 46s, Epoch [1/3], Iter [800/3722], Loss: 4.5976, Reward: -4.64, Accuracy: 0.10, PPL: 99.24
Time 3m 7s, Epoch [1/3], Iter [900/3722], Loss: 4.5496, Reward: -4.37, Accuracy: 0.10, PPL: 94.59
Time 3m 28s, Epoch [1/3], Iter [1000/3722], Loss: 4.5352, Reward: -4.55, Accuracy: 0.09, PPL: 93.24
Time 3m 49s, Epoch [1/3], Iter [1100/3722], Loss: 4.4652, Reward: -4.40, Accuracy: 0.10, PPL: 86.94
Time 4m 10s, Epoch [1/3], Iter [1200/3722], Loss: 4.5035, Reward: -4.55, Accuracy: 0.10, PPL: 90.34
Time 4m 30s, Epoch [1/3], Iter [1300/3722], Loss: 4.4542, Reward: -4.50, Accuracy: 0.09, PPL: 85.99
Time 4m 51s, Epoch [1/3], Iter [1400/3722], Loss: 4.4310, Reward: -4.41, Accuracy: 0.09, PPL: 84.02
Time 5m 12s, Epoch [1/3], Iter [1500/3722], Loss: 4.4737, Reward: -4.54, Accuracy: 0.08, PPL: 87.68
Time 5m 32s, Epoch [1/3], Iter [1600/3722], Loss: 4.4179, Reward: -4.40, Accuracy: 0.09, PPL: 82.93
Time 5m 53s, Epoch [1/3], Iter [1700/3722], Loss: 4.3852, Reward: -4.38, Accuracy: 0.07, PPL: 80.25
Time 6m 14s, Epoch [1/3], Iter [1800/3722], Loss: 4.3901, Reward: -4.37, Accuracy: 0.07, PPL: 80.65
Time 6m 34s, Epoch [1/3], Iter [1900/3722], Loss: 4.3523, Reward: -4.34, Accuracy: 0.08, PPL: 77.66
Time 6m 55s, Epoch [1/3], Iter [2000/3722], Loss: 4.3592, Reward: -4.38, Accuracy: 0.07, PPL: 78.19
Time 7m 15s, Epoch [1/3], Iter [2100/3722], Loss: 4.3127, Reward: -4.31, Accuracy: 0.07, PPL: 74.64
Time 7m 36s, Epoch [1/3], Iter [2200/3722], Loss: 4.3007, Reward: -4.24, Accuracy: 0.08, PPL: 73.75
Time 7m 57s, Epoch [1/3], Iter [2300/3722], Loss: 4.3169, Reward: -4.21, Accuracy: 0.07, PPL: 74.96
Time 8m 18s, Epoch [1/3], Iter [2400/3722], Loss: 4.2923, Reward: -4.32, Accuracy: 0.09, PPL: 73.13
Time 8m 38s, Epoch [1/3], Iter [2500/3722], Loss: 4.2648, Reward: -4.20, Accuracy: 0.09, PPL: 71.15
Time 8m 59s, Epoch [1/3], Iter [2600/3722], Loss: 4.2529, Reward: -4.41, Accuracy: 0.09, PPL: 70.31
Time 9m 19s, Epoch [1/3], Iter [2700/3722], Loss: 4.2562, Reward: -4.31, Accuracy: 0.08, PPL: 70.54
Time 9m 40s, Epoch [1/3], Iter [2800/3722], Loss: 4.2317, Reward: -4.28, Accuracy: 0.08, PPL: 68.83
Time 10m 1s, Epoch [1/3], Iter [2900/3722], Loss: 4.2675, Reward: -4.24, Accuracy: 0.09, PPL: 71.35
Time 10m 22s, Epoch [1/3], Iter [3000/3722], Loss: 4.2261, Reward: -4.26, Accuracy: 0.09, PPL: 68.45
Time 10m 42s, Epoch [1/3], Iter [3100/3722], Loss: 4.2408, Reward: -4.16, Accuracy: 0.08, PPL: 69.46
Time 11m 3s, Epoch [1/3], Iter [3200/3722], Loss: 4.2227, Reward: -4.16, Accuracy: 0.08, PPL: 68.22
Time 11m 23s, Epoch [1/3], Iter [3300/3722], Loss: 4.1993, Reward: -4.17, Accuracy: 0.08, PPL: 66.64
Time 11m 44s, Epoch [1/3], Iter [3400/3722], Loss: 4.2237, Reward: -4.10, Accuracy: 0.08, PPL: 68.28
Time 12m 5s, Epoch [1/3], Iter [3500/3722], Loss: 4.1894, Reward: -4.21, Accuracy: 0.08, PPL: 65.99
Time 12m 25s, Epoch [1/3], Iter [3600/3722], Loss: 4.1862, Reward: -4.42, Accuracy: 0.09, PPL: 65.77
Time 12m 46s, Epoch [1/3], Iter [3700/3722], Loss: 4.1476, Reward: -4.12, Accuracy: 0.07, PPL: 63.28
Validation. Time 12m 51s, PPL: 55.88
Time 13m 12s, Epoch [2/3], Iter [100/3722], Loss: 4.1585, Reward: -4.23, Accuracy: 0.08, PPL: 63.98
Time 13m 33s, Epoch [2/3], Iter [200/3722], Loss: 4.1523, Reward: -4.07, Accuracy: 0.08, PPL: 63.58
Time 13m 53s, Epoch [2/3], Iter [300/3722], Loss: 4.0868, Reward: -4.07, Accuracy: 0.08, PPL: 59.55
Time 14m 14s, Epoch [2/3], Iter [400/3722], Loss: 4.1552, Reward: -4.14, Accuracy: 0.08, PPL: 63.76
Time 14m 34s, Epoch [2/3], Iter [500/3722], Loss: 4.1562, Reward: -4.10, Accuracy: 0.08, PPL: 63.83
Time 14m 55s, Epoch [2/3], Iter [600/3722], Loss: 4.1031, Reward: -4.09, Accuracy: 0.09, PPL: 60.53
Time 15m 16s, Epoch [2/3], Iter [700/3722], Loss: 4.1089, Reward: -4.12, Accuracy: 0.08, PPL: 60.88
Time 15m 36s, Epoch [2/3], Iter [800/3722], Loss: 4.1022, Reward: -4.20, Accuracy: 0.09, PPL: 60.47
Time 15m 57s, Epoch [2/3], Iter [900/3722], Loss: 4.0670, Reward: -4.09, Accuracy: 0.08, PPL: 58.38
Time 16m 17s, Epoch [2/3], Iter [1000/3722], Loss: 4.0993, Reward: -4.02, Accuracy: 0.09, PPL: 60.30
Time 16m 38s, Epoch [2/3], Iter [1100/3722], Loss: 4.1012, Reward: -4.11, Accuracy: 0.08, PPL: 60.41
Time 16m 58s, Epoch [2/3], Iter [1200/3722], Loss: 4.0767, Reward: -4.09, Accuracy: 0.08, PPL: 58.95
Time 17m 19s, Epoch [2/3], Iter [1300/3722], Loss: 4.0535, Reward: -4.07, Accuracy: 0.09, PPL: 57.60
Time 17m 40s, Epoch [2/3], Iter [1400/3722], Loss: 4.0197, Reward: -4.06, Accuracy: 0.09, PPL: 55.69
Time 18m 0s, Epoch [2/3], Iter [1500/3722], Loss: 4.0236, Reward: -3.88, Accuracy: 0.09, PPL: 55.90
Time 18m 21s, Epoch [2/3], Iter [1600/3722], Loss: 4.0014, Reward: -3.98, Accuracy: 0.09, PPL: 54.67
Time 18m 41s, Epoch [2/3], Iter [1700/3722], Loss: 4.0349, Reward: -3.99, Accuracy: 0.09, PPL: 56.54
Time 19m 2s, Epoch [2/3], Iter [1800/3722], Loss: 4.0778, Reward: -4.14, Accuracy: 0.09, PPL: 59.02
Time 19m 23s, Epoch [2/3], Iter [1900/3722], Loss: 4.0069, Reward: -3.86, Accuracy: 0.09, PPL: 54.97
Time 19m 43s, Epoch [2/3], Iter [2000/3722], Loss: 4.0076, Reward: -3.98, Accuracy: 0.09, PPL: 55.02
Time 20m 4s, Epoch [2/3], Iter [2100/3722], Loss: 4.0011, Reward: -4.00, Accuracy: 0.09, PPL: 54.66
Time 20m 24s, Epoch [2/3], Iter [2200/3722], Loss: 3.9604, Reward: -3.86, Accuracy: 0.09, PPL: 52.48
Time 20m 45s, Epoch [2/3], Iter [2300/3722], Loss: 3.9285, Reward: -3.91, Accuracy: 0.09, PPL: 50.83
Time 21m 5s, Epoch [2/3], Iter [2400/3722], Loss: 3.9829, Reward: -4.03, Accuracy: 0.09, PPL: 53.67
Time 21m 26s, Epoch [2/3], Iter [2500/3722], Loss: 3.9616, Reward: -3.98, Accuracy: 0.09, PPL: 52.54
Time 21m 47s, Epoch [2/3], Iter [2600/3722], Loss: 3.9555, Reward: -3.92, Accuracy: 0.09, PPL: 52.22
Time 22m 7s, Epoch [2/3], Iter [2700/3722], Loss: 3.9216, Reward: -3.87, Accuracy: 0.09, PPL: 50.48
Time 22m 27s, Epoch [2/3], Iter [2800/3722], Loss: 3.9385, Reward: -3.87, Accuracy: 0.08, PPL: 51.34
Time 22m 48s, Epoch [2/3], Iter [2900/3722], Loss: 3.9120, Reward: -3.88, Accuracy: 0.09, PPL: 50.00
Time 23m 9s, Epoch [2/3], Iter [3000/3722], Loss: 3.8849, Reward: -3.73, Accuracy: 0.09, PPL: 48.66
Time 23m 29s, Epoch [2/3], Iter [3100/3722], Loss: 3.8937, Reward: -3.92, Accuracy: 0.09, PPL: 49.09
Time 23m 50s, Epoch [2/3], Iter [3200/3722], Loss: 3.8962, Reward: -3.81, Accuracy: 0.09, PPL: 49.21
Time 24m 10s, Epoch [2/3], Iter [3300/3722], Loss: 3.9335, Reward: -3.96, Accuracy: 0.09, PPL: 51.08
Time 24m 31s, Epoch [2/3], Iter [3400/3722], Loss: 3.8959, Reward: -3.82, Accuracy: 0.08, PPL: 49.20
Time 24m 51s, Epoch [2/3], Iter [3500/3722], Loss: 3.8899, Reward: -3.93, Accuracy: 0.08, PPL: 48.90
Time 25m 12s, Epoch [2/3], Iter [3600/3722], Loss: 3.8849, Reward: -3.99, Accuracy: 0.09, PPL: 48.66
Time 25m 32s, Epoch [2/3], Iter [3700/3722], Loss: 3.8639, Reward: -3.88, Accuracy: 0.08, PPL: 47.65
Validation. Time 25m 38s, PPL: 40.09
Time 25m 58s, Epoch [3/3], Iter [100/3722], Loss: 3.8577, Reward: -3.83, Accuracy: 0.08, PPL: 47.36
Time 26m 19s, Epoch [3/3], Iter [200/3722], Loss: 3.8142, Reward: -3.81, Accuracy: 0.08, PPL: 45.34
Time 26m 39s, Epoch [3/3], Iter [300/3722], Loss: 3.8046, Reward: -3.79, Accuracy: 0.08, PPL: 44.91
Time 27m 0s, Epoch [3/3], Iter [400/3722], Loss: 3.8631, Reward: -3.84, Accuracy: 0.09, PPL: 47.61
Time 27m 20s, Epoch [3/3], Iter [500/3722], Loss: 3.7657, Reward: -3.65, Accuracy: 0.08, PPL: 43.20
Time 27m 41s, Epoch [3/3], Iter [600/3722], Loss: 3.8064, Reward: -3.88, Accuracy: 0.08, PPL: 44.99
Time 28m 1s, Epoch [3/3], Iter [700/3722], Loss: 3.8222, Reward: -3.83, Accuracy: 0.08, PPL: 45.71
Time 28m 22s, Epoch [3/3], Iter [800/3722], Loss: 3.7658, Reward: -3.73, Accuracy: 0.08, PPL: 43.20
Time 28m 43s, Epoch [3/3], Iter [900/3722], Loss: 3.7646, Reward: -3.89, Accuracy: 0.09, PPL: 43.15
Time 29m 3s, Epoch [3/3], Iter [1000/3722], Loss: 3.7730, Reward: -3.87, Accuracy: 0.09, PPL: 43.51
Time 29m 24s, Epoch [3/3], Iter [1100/3722], Loss: 3.7697, Reward: -3.75, Accuracy: 0.09, PPL: 43.37
Time 29m 45s, Epoch [3/3], Iter [1200/3722], Loss: 3.7961, Reward: -3.91, Accuracy: 0.09, PPL: 44.53
Time 30m 5s, Epoch [3/3], Iter [1300/3722], Loss: 3.7938, Reward: -3.81, Accuracy: 0.08, PPL: 44.43
Time 30m 26s, Epoch [3/3], Iter [1400/3722], Loss: 3.7953, Reward: -3.78, Accuracy: 0.08, PPL: 44.49
Time 30m 47s, Epoch [3/3], Iter [1500/3722], Loss: 3.7460, Reward: -3.71, Accuracy: 0.08, PPL: 42.35
Time 31m 7s, Epoch [3/3], Iter [1600/3722], Loss: 3.7971, Reward: -3.72, Accuracy: 0.09, PPL: 44.57
Time 31m 28s, Epoch [3/3], Iter [1700/3722], Loss: 3.7173, Reward: -3.74, Accuracy: 0.09, PPL: 41.15
Time 31m 48s, Epoch [3/3], Iter [1800/3722], Loss: 3.6935, Reward: -3.66, Accuracy: 0.09, PPL: 40.19
Time 32m 9s, Epoch [3/3], Iter [1900/3722], Loss: 3.7321, Reward: -3.69, Accuracy: 0.09, PPL: 41.77
Time 32m 29s, Epoch [3/3], Iter [2000/3722], Loss: 3.7699, Reward: -3.72, Accuracy: 0.08, PPL: 43.38
Time 32m 50s, Epoch [3/3], Iter [2100/3722], Loss: 3.7425, Reward: -3.71, Accuracy: 0.09, PPL: 42.21
Time 33m 11s, Epoch [3/3], Iter [2200/3722], Loss: 3.7675, Reward: -3.88, Accuracy: 0.09, PPL: 43.27
Time 33m 32s, Epoch [3/3], Iter [2300/3722], Loss: 3.7315, Reward: -3.75, Accuracy: 0.10, PPL: 41.74
Time 33m 52s, Epoch [3/3], Iter [2400/3722], Loss: 3.7083, Reward: -3.73, Accuracy: 0.09, PPL: 40.78
Time 34m 12s, Epoch [3/3], Iter [2500/3722], Loss: 3.6852, Reward: -3.67, Accuracy: 0.09, PPL: 39.85
Time 34m 33s, Epoch [3/3], Iter [2600/3722], Loss: 3.7150, Reward: -3.59, Accuracy: 0.09, PPL: 41.06
Time 34m 54s, Epoch [3/3], Iter [2700/3722], Loss: 3.6835, Reward: -3.87, Accuracy: 0.09, PPL: 39.79
Time 35m 14s, Epoch [3/3], Iter [2800/3722], Loss: 3.6731, Reward: -3.59, Accuracy: 0.10, PPL: 39.37
Time 35m 35s, Epoch [3/3], Iter [2900/3722], Loss: 3.7908, Reward: -3.79, Accuracy: 0.10, PPL: 44.29
Time 35m 55s, Epoch [3/3], Iter [3000/3722], Loss: 3.6891, Reward: -3.72, Accuracy: 0.09, PPL: 40.01
Time 36m 16s, Epoch [3/3], Iter [3100/3722], Loss: 3.6566, Reward: -3.63, Accuracy: 0.09, PPL: 38.73
Time 36m 36s, Epoch [3/3], Iter [3200/3722], Loss: 3.6880, Reward: -3.71, Accuracy: 0.10, PPL: 39.96
Time 36m 57s, Epoch [3/3], Iter [3300/3722], Loss: 3.6868, Reward: -3.67, Accuracy: 0.10, PPL: 39.92
Time 37m 17s, Epoch [3/3], Iter [3400/3722], Loss: 3.6719, Reward: -3.55, Accuracy: 0.10, PPL: 39.33
Time 37m 38s, Epoch [3/3], Iter [3500/3722], Loss: 3.6522, Reward: -3.77, Accuracy: 0.11, PPL: 38.56
Time 37m 59s, Epoch [3/3], Iter [3600/3722], Loss: 3.6292, Reward: -3.66, Accuracy: 0.11, PPL: 37.68
Time 38m 19s, Epoch [3/3], Iter [3700/3722], Loss: 3.6587, Reward: -3.66, Accuracy: 0.10, PPL: 38.81
Validation. Time 38m 25s, PPL: 31.71
Traceback (most recent call last):
  File "main.py", line 201, in <module>
    _,wordlist,_ = model.predict2(x_de,beamsz=100,gen_len=3)
  File "/mnt/trunk/cs287-s18/HW3/models.py", line 183, in predict2
    h, c = masterheap.get_hiddens() # (nlayers*ndirections,beamsz,hiddensz),(nlayers*ndirections,beamsz,hiddensz)
  File "/mnt/trunk/cs287-s18/HW3/models.py", line 33, in get_hiddens
    return tuple(Variable(self.hiddens[0]),Variable(self.hiddens[1]))
TypeError: tuple() takes at most 1 argument (2 given)
[01;32melbertgong@nlpfinal[00m:[01;34m/mnt/trunk/cs287-s18/HW3[00m$ git status
On branch master
Your branch is up-to-date with 'origin/master'.
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git checkout -- <file>..." to discard changes in working directory)

	[31mmodified:   models.py[m
	[31mmodified:   preds.csv[m

Untracked files:
  (use "git add <file>..." to include in what will be committed)

	[31mscreenlog.0[m

no changes added to commit (use "git add" and/or "git commit -a")
[01;32melbertgong@nlpfinal[00m:[01;34m/mnt/trunk/cs287-s18/HW3[00m$ exit
exit
