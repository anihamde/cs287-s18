from models import AttnNetwork, CandList, S2S
from helpers import asMinutes, timeSince, escape
model = AttnNetwork()
model.cuda()
model.load_state_dict(torch.load('../../models/HW3/model.pkl'))

x_de = batch.src.transpose(1,0).cuda() # bs,n_de
x_en = batch.trg.transpose(1,0).cuda() # bs,n_en

y_pred = model.predict(x_de, attn_type) # bs,max_len

# teacher forcing predict
def predicttf(self, x_de, x_en attn_type = "hard"):
    bs = x_de.size(0)
    emb_de = self.embedding_de(x_de) # bs,n_de,word_dim
    emb_en = self.embedding_en(x_en) # bs,n_en,word_dim
    h = Variable(torch.zeros(1, bs, self.hidden_dim).cuda())
    c = Variable(torch.zeros(1, bs, self.hidden_dim).cuda())
    enc_h, _ = self.encoder(emb_de, (h, c))
    dec_h, _ = self.decoder(emb_en, (h, c))
    # all the same. enc_h is bs,n_de,hiddensz*ndirections. h and c are both nlayers*ndirections,bs,hiddensz
    scores = torch.bmm(enc_h, dec_h.transpose(1,2))
    # (bs,n_de,hiddensz*ndirections) * (bs,hiddensz*ndirections,n_en) = (bs,n_de,n_en)
    y = [Variable(torch.cuda.LongTensor([sos_token]*bs))] # bs
    self.attn = []
    n_en = MAX_LEN+1 # to be safe
    for t in range(n_en): # generate some english.
        attn_dist = F.softmax(scores[:,:,t],dim=1) # bs,n_de
        self.attn.append(attn_dist.data)
        if attn_type == "hard":
            _, argmax = attn_dist.max(1) # bs. for each batch, select most likely german word to pay attention to
            one_hot = Variable(torch.zeros_like(attn_dist.data).scatter_(-1, argmax.data.unsqueeze(1), 1).cuda())
            context = torch.bmm(one_hot.unsqueeze(1), enc_h).squeeze(1)
        else:
            context = torch.bmm(attn_dist.unsqueeze(1), enc_h).squeeze(1)
        # the difference btwn hard and soft is just whether we use a one_hot or a distribution
        # context is bs,hiddensz*ndirections
        pred = self.vocab_layer(torch.cat([dec_h[:,t,:], context], 1)) # bs,len(EN.vocab)
        _, next_token = pred.max(1) # bs
        y.append(next_token)
    self.attn = torch.stack(self.attn, 0).transpose(0, 1) # bs,n_en,n_de (for visualization!)
    return torch.stack(y, 0).transpose(0, 1) # bs,n_en


bs = x_de.size(0)
emb_de = model.embedding_de(x_de) # bs,n_de,word_dim
h = Variable(torch.zeros(1, bs, model.hidden_dim).cuda())
c = Variable(torch.zeros(1, bs, model.hidden_dim).cuda())
enc_h, _ = model.encoder(emb_de, (h, c))
# all the same. enc_h is bs,n_de,hiddensz*ndirections. h and c are both nlayers*ndirections,bs,hiddensz
y = [Variable(torch.cuda.LongTensor([sos_token]*bs))] # bs
model.attn = []
n_en = MAX_LEN+1 # to be safe

# PREDICT
# t = 0
# emb_t = model.embedding_en(y[-1]) # embed the last thing we generated. bs
# dec_h, (h, c) = model.decoder(emb_t.unsqueeze(1), (h, c)) # dec_h is bs,1,hiddensz*ndirections (batch_first=True)
# scores = torch.bmm(enc_h, dec_h.transpose(1,2)).squeeze(2)
# # (bs,n_de,hiddensz*ndirections) * (bs,hiddensz*ndirections,1) = (bs,n_de,1). squeeze to bs,n_de
# attn_dist = F.softmax(scores,dim=1)
# model.attn.append(attn_dist.data)
# if attn_type == "hard":
#     _, argmax = attn_dist.max(1) # bs. for each batch, select most likely german word to pay attention to
#     one_hot = Variable(torch.zeros_like(attn_dist.data).scatter_(-1, argmax.data.unsqueeze(1), 1).cuda())
#     context = torch.bmm(one_hot.unsqueeze(1), enc_h).squeeze(1)
# else:
#     context = torch.bmm(attn_dist.unsqueeze(1), enc_h).squeeze(1)
# # the difference btwn hard and soft is just whether we use a one_hot or a distribution
# # context is bs,hiddensz*ndirections
# pred = model.vocab_layer(torch.cat([dec_h.squeeze(1), context], 1)) # bs,len(EN.vocab)
# _, next_token = pred.max(1) # bs
# y.append(next_token)








bs = x_de.size(0)
# x_de is bs,n_de. x_en is bs,n_en
emb_de = model.embedding_de(x_de) # bs,n_de,word_dim
emb_en = model.embedding_en(x_en) # bs,n_en,word_dim
h0 = Variable(torch.zeros(1, bs, model.hidden_dim).cuda()) 
c0 = Variable(torch.zeros(1, bs, model.hidden_dim).cuda())
# hidden vars have dimension nlayers*ndirections,bs,hiddensz
enc_h, _ = model.encoder(emb_de, (h0, c0))
# enc_h is bs,n_de,hiddensz*ndirections. ordering is different from last week because batch_first=True
dec_h, _ = model.decoder(emb_en, (h0, c0))
# dec_h is bs,n_en,hidden_size*ndirections
# we've gotten our encoder/decoder hidden states so we are ready to do attention
# first let's get all our scores, which we can do easily since we are using dot-prod attention
scores = torch.bmm(enc_h, dec_h.transpose(1,2))
scores = F.softmax(scores,dim=1)
# (bs,n_de,hiddensz*ndirections) * (bs,hiddensz*ndirections,n_en) = (bs,n_de,n_en)
reinforce_loss = 0 # we only use this variable for hard attention
loss = 0
avg_reward = 0

t = 0

attn_dist = scores[:, :, t] # bs,n_de. these are the alphas (attention scores for each german word)
if attn_type == "hard":
    cat = torch.distributions.Categorical(attn_dist) 
    attn_samples = cat.sample() # bs. each element is a sample from categorical distribution
    one_hot = Variable(torch.zeros_like(attn_dist.data).scatter_(-1, attn_samples.data.unsqueeze(1), 1).cuda()) # bs,n_de
    # made a bunch of one-hot vectors
    context = torch.bmm(one_hot.unsqueeze(1), enc_h).squeeze(1)
    # now we use the one-hot vectors to select correct hidden vectors from enc_h
    # (bs,1,n_de) * (bs,n_de,hiddensz*ndirections) = (bs,1,hiddensz*ndirections). squeeze to bs,hiddensz*ndirections
else:
    context = torch.bmm(attn_dist.unsqueeze(1), enc_h).squeeze(1) # same dimensions
# context is bs,hidden_size*ndirections
# the rnn output and the context together make the decoder "hidden state", which is bs,2*hidden_size*ndirections
pred = self.vocab_layer(torch.cat([dec_h[:,t,:], context], 1)) # bs,len(EN.vocab)
y = x_en[:, t+1] # bs. these are our labels
no_pad = (y != pad_token) # exclude english padding tokens
reward = torch.gather(pred, 1, y.unsqueeze(1)) # bs,1
# reward[i,1] = pred[i,y[i]]. this gets log prob of correct word for each batch. similar to -crossentropy
reward = reward.squeeze(1)[no_pad] # less than bs
avg_reward += reward.data.mean()
if attn_type == "hard":
    reinforce_loss -= (cat.log_prob(attn_samples[no_pad]) * (reward-self.baseline).detach()).mean() 
    # reinforce rule (just read the formula), with special baseline
loss -= reward.mean() # minimizing loss is maximizing reward