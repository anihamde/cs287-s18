[01;32melbertgong@nlpfinal[00m:[01;34m/mnt/trunk/cs287-s18/HW2[00m$ exitpython3 lstm.py -m '../../models/HW2/lstm2.py'
len(train) 1
len(TEXT.vocab) 10001
Size of text batch [max bptt length, batch size] torch.Size([32, 10])
Second in batch Variable containing:
    8
  202
   77
    5
  183
  561
 3837
   18
  975
  976
    7
  943
    5
  157
   78
 1571
  289
  645
    3
   30
  132
    0
   20
    2
  273
 7821
   17
    9
  117
 2815
  969
    6
[torch.LongTensor of size 32]

Converted back to string:  in part because of buy programs generated by stock-index arbitrage a form of program trading involving futures contracts <eos> but interest <unk> as the day wore on and investors looked ahead to
Converted back to string:  the release later this week of two important economic reports <eos> the first is wednesday 's survey of purchasing managers considered a good indicator of how the nation 's manufacturing sector fared
Word embeddings size  torch.Size([10001, 300])
REMINDER!!! Did you create ../../models/HW2?????
CUDA is available, assigning to GPU.
Epoch [1/10], Iter [100/2905] Loss: 20.6579
Epoch [1/10], Iter [200/2905] Loss: 20.2635
Epoch [1/10], Iter [300/2905] Loss: 19.7266
Epoch [1/10], Iter [400/2905] Loss: 19.2212
Epoch [1/10], Iter [500/2905] Loss: 18.8579
Epoch [1/10], Iter [600/2905] Loss: 18.1108
Epoch [1/10], Iter [700/2905] Loss: 17.5113
Epoch [1/10], Iter [800/2905] Loss: 17.0567
Epoch [1/10], Iter [900/2905] Loss: 16.6427
Epoch [1/10], Iter [1000/2905] Loss: 16.3739
Epoch [1/10], Iter [1100/2905] Loss: 16.1345
Epoch [1/10], Iter [1200/2905] Loss: 15.8453
Epoch [1/10], Iter [1300/2905] Loss: 15.6009
Epoch [1/10], Iter [1400/2905] Loss: 15.4293
Epoch [1/10], Iter [1500/2905] Loss: 15.2839
Epoch [1/10], Iter [1600/2905] Loss: 14.9294
Epoch [1/10], Iter [1700/2905] Loss: 14.8209
Epoch [1/10], Iter [1800/2905] Loss: 14.6759
Epoch [1/10], Iter [1900/2905] Loss: 14.6093
Epoch [1/10], Iter [2000/2905] Loss: 14.3906
Epoch [1/10], Iter [2100/2905] Loss: 14.4811
Epoch [1/10], Iter [2200/2905] Loss: 14.3095
Epoch [1/10], Iter [2300/2905] Loss: 14.1965
Epoch [1/10], Iter [2400/2905] Loss: 14.1101
Epoch [1/10], Iter [2500/2905] Loss: 14.0638
Epoch [1/10], Iter [2600/2905] Loss: 13.8756
Epoch [1/10], Iter [2700/2905] Loss: 13.6718
Epoch [1/10], Iter [2800/2905] Loss: 13.5442
Epoch [1/10], Iter [2900/2905] Loss: 13.4417
Val acc, prec, ppl 0.0032034632034632035 0.043412357574062685 19756.431640625
Epoch [2/10], Iter [100/2905] Loss: 13.2946
Epoch [2/10], Iter [200/2905] Loss: 13.2413
Epoch [2/10], Iter [300/2905] Loss: 13.2543
Epoch [2/10], Iter [400/2905] Loss: 13.1146
Epoch [2/10], Iter [500/2905] Loss: 12.9282
Epoch [2/10], Iter [600/2905] Loss: 12.7395
Epoch [2/10], Iter [700/2905] Loss: 12.5920
Epoch [2/10], Iter [800/2905] Loss: 12.4677
Epoch [2/10], Iter [900/2905] Loss: 12.2954
Epoch [2/10], Iter [1000/2905] Loss: 12.2492
Epoch [2/10], Iter [1100/2905] Loss: 12.2093
Epoch [2/10], Iter [1200/2905] Loss: 12.0817
Epoch [2/10], Iter [1300/2905] Loss: 11.9667
Epoch [2/10], Iter [1400/2905] Loss: 11.9208
Epoch [2/10], Iter [1500/2905] Loss: 11.8762
Epoch [2/10], Iter [1600/2905] Loss: 11.6385
Epoch [2/10], Iter [1700/2905] Loss: 11.6435
Epoch [2/10], Iter [1800/2905] Loss: 11.5730
Epoch [2/10], Iter [1900/2905] Loss: 11.6059
Epoch [2/10], Iter [2000/2905] Loss: 11.4318
Epoch [2/10], Iter [2100/2905] Loss: 11.5665
Epoch [2/10], Iter [2200/2905] Loss: 11.4410
Epoch [2/10], Iter [2300/2905] Loss: 11.3646
Epoch [2/10], Iter [2400/2905] Loss: 11.3354
Epoch [2/10], Iter [2500/2905] Loss: 11.3778
Epoch [2/10], Iter [2600/2905] Loss: 11.2375
Epoch [2/10], Iter [2700/2905] Loss: 11.0524
Epoch [2/10], Iter [2800/2905] Loss: 11.0051
Epoch [2/10], Iter [2900/2905] Loss: 10.9051
Val acc, prec, ppl 0.0013852813852813853 0.024660309587012638 25874.53515625
Epoch [3/10], Iter [100/2905] Loss: 10.8166
Epoch [3/10], Iter [200/2905] Loss: 10.8285
Epoch [3/10], Iter [300/2905] Loss: 10.9296
Epoch [3/10], Iter [400/2905] Loss: 10.8416
Epoch [3/10], Iter [500/2905] Loss: 10.7159
Epoch [3/10], Iter [600/2905] Loss: 10.5741
Epoch [3/10], Iter [700/2905] Loss: 10.4599
Epoch [3/10], Iter [800/2905] Loss: 10.4158
Epoch [3/10], Iter [900/2905] Loss: 10.3016
Epoch [3/10], Iter [1000/2905] Loss: 10.3219
Epoch [3/10], Iter [1100/2905] Loss: 10.3205
Epoch [3/10], Iter [1200/2905] Loss: 10.2554
Epoch [3/10], Iter [1300/2905] Loss: 10.1675
Epoch [3/10], Iter [1400/2905] Loss: 10.1440
Epoch [3/10], Iter [1500/2905] Loss: 10.1478
Epoch [3/10], Iter [1600/2905] Loss: 9.9661
Epoch [3/10], Iter [1700/2905] Loss: 9.9763
Epoch [3/10], Iter [1800/2905] Loss: 9.9325
Epoch [3/10], Iter [1900/2905] Loss: 9.9813
Epoch [3/10], Iter [2000/2905] Loss: 9.8270
Epoch [3/10], Iter [2100/2905] Loss: 9.9614
Epoch [3/10], Iter [2200/2905] Loss: 9.8829
Epoch [3/10], Iter [2300/2905] Loss: 9.8216
Epoch [3/10], Iter [2400/2905] Loss: 9.8327
Epoch [3/10], Iter [2500/2905] Loss: 9.8920
Epoch [3/10], Iter [2600/2905] Loss: 9.7834
Epoch [3/10], Iter [2700/2905] Loss: 9.6241
Epoch [3/10], Iter [2800/2905] Loss: 9.5847
Epoch [3/10], Iter [2900/2905] Loss: 9.4696
Val acc, prec, ppl 0.0009090909090909091 0.015675066203298248 32940.1875
Epoch [4/10], Iter [100/2905] Loss: 9.3649
Epoch [4/10], Iter [200/2905] Loss: 9.3570
Epoch [4/10], Iter [300/2905] Loss: 9.4466
Epoch [4/10], Iter [400/2905] Loss: 9.3731
Epoch [4/10], Iter [500/2905] Loss: 9.2656
Epoch [4/10], Iter [600/2905] Loss: 9.1620
Epoch [4/10], Iter [700/2905] Loss: 9.0781
Epoch [4/10], Iter [800/2905] Loss: 9.0703
Epoch [4/10], Iter [900/2905] Loss: 8.9926
Epoch [4/10], Iter [1000/2905] Loss: 9.0641
Epoch [4/10], Iter [1100/2905] Loss: 9.0746
Epoch [4/10], Iter [1200/2905] Loss: 9.0400
Epoch [4/10], Iter [1300/2905] Loss: 8.9528
Epoch [4/10], Iter [1400/2905] Loss: 8.9430
Epoch [4/10], Iter [1500/2905] Loss: 8.9688
Epoch [4/10], Iter [1600/2905] Loss: 8.8159
Epoch [4/10], Iter [1700/2905] Loss: 8.8215
Epoch [4/10], Iter [1800/2905] Loss: 8.8009
Epoch [4/10], Iter [1900/2905] Loss: 8.8371
Epoch [4/10], Iter [2000/2905] Loss: 8.6699
Epoch [4/10], Iter [2100/2905] Loss: 8.8116
Epoch [4/10], Iter [2200/2905] Loss: 8.7594
Epoch [4/10], Iter [2300/2905] Loss: 8.6794
Epoch [4/10], Iter [2400/2905] Loss: 8.7088
Epoch [4/10], Iter [2500/2905] Loss: 8.7660
Epoch [4/10], Iter [2600/2905] Loss: 8.6522
Epoch [4/10], Iter [2700/2905] Loss: 8.4836
Epoch [4/10], Iter [2800/2905] Loss: 8.4687
Epoch [4/10], Iter [2900/2905] Loss: 8.3534
Val acc, prec, ppl 0.0010822510822510823 0.013539113410268769 41467.94921875
Epoch [5/10], Iter [100/2905] Loss: 8.2968
Epoch [5/10], Iter [200/2905] Loss: 8.3159
Epoch [5/10], Iter [300/2905] Loss: 8.4230
Epoch [5/10], Iter [400/2905] Loss: 8.3821
Epoch [5/10], Iter [500/2905] Loss: 8.3182
Epoch [5/10], Iter [600/2905] Loss: 8.1841
Epoch [5/10], Iter [700/2905] Loss: 8.0963
Epoch [5/10], Iter [800/2905] Loss: 8.0849
Epoch [5/10], Iter [900/2905] Loss: 7.9907
Epoch [5/10], Iter [1000/2905] Loss: 8.0552
Epoch [5/10], Iter [1100/2905] Loss: 8.0944
Epoch [5/10], Iter [1200/2905] Loss: 8.0670
Epoch [5/10], Iter [1300/2905] Loss: 8.0024
Epoch [5/10], Iter [1400/2905] Loss: 8.0052
Epoch [5/10], Iter [1500/2905] Loss: 8.0197
Epoch [5/10], Iter [1600/2905] Loss: 7.8872
Epoch [5/10], Iter [1700/2905] Loss: 7.9021
Epoch [5/10], Iter [1800/2905] Loss: 7.8986
Epoch [5/10], Iter [1900/2905] Loss: 7.9510
Epoch [5/10], Iter [2000/2905] Loss: 7.7987
Epoch [5/10], Iter [2100/2905] Loss: 7.9292
Epoch [5/10], Iter [2200/2905] Loss: 7.8808
Epoch [5/10], Iter [2300/2905] Loss: 7.8017
Epoch [5/10], Iter [2400/2905] Loss: 7.8285
Epoch [5/10], Iter [2500/2905] Loss: 7.9113
Epoch [5/10], Iter [2600/2905] Loss: 7.8129
Epoch [5/10], Iter [2700/2905] Loss: 7.6774
Epoch [5/10], Iter [2800/2905] Loss: 7.6583
Epoch [5/10], Iter [2900/2905] Loss: 7.5665
Val acc, prec, ppl 0.0006926406926406926 0.010571727035526847 50188.234375
Epoch [6/10], Iter [100/2905] Loss: 7.5053
Epoch [6/10], Iter [200/2905] Loss: 7.5158
Epoch [6/10], Iter [300/2905] Loss: 7.5965
Epoch [6/10], Iter [400/2905] Loss: 7.5726
Epoch [6/10], Iter [500/2905] Loss: 7.5066
Epoch [6/10], Iter [600/2905] Loss: 7.3731
Epoch [6/10], Iter [700/2905] Loss: 7.2957
Epoch [6/10], Iter [800/2905] Loss: 7.3117
Epoch [6/10], Iter [900/2905] Loss: 7.2216
Epoch [6/10], Iter [1000/2905] Loss: 7.2783
Epoch [6/10], Iter [1100/2905] Loss: 7.3165
Epoch [6/10], Iter [1200/2905] Loss: 7.3104
Epoch [6/10], Iter [1300/2905] Loss: 7.2403
Epoch [6/10], Iter [1400/2905] Loss: 7.2494
Epoch [6/10], Iter [1500/2905] Loss: 7.2652
Epoch [6/10], Iter [1600/2905] Loss: 7.1697
Epoch [6/10], Iter [1700/2905] Loss: 7.1694
Epoch [6/10], Iter [1800/2905] Loss: 7.1701
Epoch [6/10], Iter [1900/2905] Loss: 7.2154
Epoch [6/10], Iter [2000/2905] Loss: 7.0741
Epoch [6/10], Iter [2100/2905] Loss: 7.1913
Epoch [6/10], Iter [2200/2905] Loss: 7.1893
Epoch [6/10], Iter [2300/2905] Loss: 7.1056
Epoch [6/10], Iter [2400/2905] Loss: 7.1338
Epoch [6/10], Iter [2500/2905] Loss: 7.2185
Epoch [6/10], Iter [2600/2905] Loss: 7.1227
Epoch [6/10], Iter [2700/2905] Loss: 6.9997
Epoch [6/10], Iter [2800/2905] Loss: 7.0085
Epoch [6/10], Iter [2900/2905] Loss: 6.9295
Val acc, prec, ppl 0.0005194805194805195 0.009260137628489758 59513.609375
Epoch [7/10], Iter [100/2905] Loss: 6.8566
Epoch [7/10], Iter [200/2905] Loss: 6.8764
Epoch [7/10], Iter [300/2905] Loss: 6.9432
Epoch [7/10], Iter [400/2905] Loss: 6.9214
Epoch [7/10], Iter [500/2905] Loss: 6.8472
Epoch [7/10], Iter [600/2905] Loss: 6.7547
Epoch [7/10], Iter [700/2905] Loss: 6.6792
Epoch [7/10], Iter [800/2905] Loss: 6.6922
Epoch [7/10], Iter [900/2905] Loss: 6.6160
Epoch [7/10], Iter [1000/2905] Loss: 6.7095
Epoch [7/10], Iter [1100/2905] Loss: 6.7713
Epoch [7/10], Iter [1200/2905] Loss: 6.7685
Epoch [7/10], Iter [1300/2905] Loss: 6.7282
Epoch [7/10], Iter [1400/2905] Loss: 6.7418
Epoch [7/10], Iter [1500/2905] Loss: 6.7623
Epoch [7/10], Iter [1600/2905] Loss: 6.6484
Epoch [7/10], Iter [1700/2905] Loss: 6.6500
Epoch [7/10], Iter [1800/2905] Loss: 6.6235
Epoch [7/10], Iter [1900/2905] Loss: 6.6682
Epoch [7/10], Iter [2000/2905] Loss: 6.5355
Epoch [7/10], Iter [2100/2905] Loss: 6.6546
Epoch [7/10], Iter [2200/2905] Loss: 6.6619
Epoch [7/10], Iter [2300/2905] Loss: 6.6030
Epoch [7/10], Iter [2400/2905] Loss: 6.6269
Epoch [7/10], Iter [2500/2905] Loss: 6.7059
Epoch [7/10], Iter [2600/2905] Loss: 6.6129
Epoch [7/10], Iter [2700/2905] Loss: 6.4782
Epoch [7/10], Iter [2800/2905] Loss: 6.4719
Epoch [7/10], Iter [2900/2905] Loss: 6.4091
Val acc, prec, ppl 0.0005627705627705628 0.008692326401703027 68384.875
Epoch [8/10], Iter [100/2905] Loss: 6.3422
Epoch [8/10], Iter [200/2905] Loss: 6.3464
Epoch [8/10], Iter [300/2905] Loss: 6.4098
Epoch [8/10], Iter [400/2905] Loss: 6.4100
Epoch [8/10], Iter [500/2905] Loss: 6.3345
Epoch [8/10], Iter [600/2905] Loss: 6.2310
Epoch [8/10], Iter [700/2905] Loss: 6.1827
Epoch [8/10], Iter [800/2905] Loss: 6.2188
Epoch [8/10], Iter [900/2905] Loss: 6.1222
Epoch [8/10], Iter [1000/2905] Loss: 6.2118
Epoch [8/10], Iter [1100/2905] Loss: 6.2668
Epoch [8/10], Iter [1200/2905] Loss: 6.2686
Epoch [8/10], Iter [1300/2905] Loss: 6.1931
Epoch [8/10], Iter [1400/2905] Loss: 6.2121
Epoch [8/10], Iter [1500/2905] Loss: 6.2088
Epoch [8/10], Iter [1600/2905] Loss: 6.1281
Epoch [8/10], Iter [1700/2905] Loss: 6.1411
Epoch [8/10], Iter [1800/2905] Loss: 6.1496
Epoch [8/10], Iter [1900/2905] Loss: 6.1872
Epoch [8/10], Iter [2000/2905] Loss: 6.0745
Epoch [8/10], Iter [2100/2905] Loss: 6.1744
Epoch [8/10], Iter [2200/2905] Loss: 6.1499
Epoch [8/10], Iter [2300/2905] Loss: 6.0771
Epoch [8/10], Iter [2400/2905] Loss: 6.1061
Epoch [8/10], Iter [2500/2905] Loss: 6.1769
Epoch [8/10], Iter [2600/2905] Loss: 6.1047
Epoch [8/10], Iter [2700/2905] Loss: 5.9950
Epoch [8/10], Iter [2800/2905] Loss: 6.0113
Epoch [8/10], Iter [2900/2905] Loss: 5.9570
Val acc, prec, ppl 0.0003463203463203463 0.006894156596425807 85135.25
Epoch [9/10], Iter [100/2905] Loss: 5.8841
Epoch [9/10], Iter [200/2905] Loss: 5.8848
Epoch [9/10], Iter [300/2905] Loss: 5.9586
Epoch [9/10], Iter [400/2905] Loss: 5.9578
Epoch [9/10], Iter [500/2905] Loss: 5.9036
Epoch [9/10], Iter [600/2905] Loss: 5.8325
Epoch [9/10], Iter [700/2905] Loss: 5.7847
Epoch [9/10], Iter [800/2905] Loss: 5.7967
Epoch [9/10], Iter [900/2905] Loss: 5.7098
Epoch [9/10], Iter [1000/2905] Loss: 5.7799
Epoch [9/10], Iter [1100/2905] Loss: 5.8220
Epoch [9/10], Iter [1200/2905] Loss: 5.8249
Epoch [9/10], Iter [1300/2905] Loss: 5.7625
Epoch [9/10], Iter [1400/2905] Loss: 5.7764
Epoch [9/10], Iter [1500/2905] Loss: 5.7975
Epoch [9/10], Iter [1600/2905] Loss: 5.7362
Epoch [9/10], Iter [1700/2905] Loss: 5.7380
Epoch [9/10], Iter [1800/2905] Loss: 5.7586
Epoch [9/10], Iter [1900/2905] Loss: 5.7979
Epoch [9/10], Iter [2000/2905] Loss: 5.6759
Epoch [9/10], Iter [2100/2905] Loss: 5.7798
Epoch [9/10], Iter [2200/2905] Loss: 5.7601
Epoch [9/10], Iter [2300/2905] Loss: 5.7031
Epoch [9/10], Iter [2400/2905] Loss: 5.7644
Epoch [9/10], Iter [2500/2905] Loss: 5.8576
Epoch [9/10], Iter [2600/2905] Loss: 5.7728
Epoch [9/10], Iter [2700/2905] Loss: 5.6942
Epoch [9/10], Iter [2800/2905] Loss: 5.7057
Epoch [9/10], Iter [2900/2905] Loss: 5.6246
Val acc, prec, ppl 0.00030303030303030303 0.0068957317697924455 96774.7734375
Epoch [10/10], Iter [100/2905] Loss: 5.5264
Epoch [10/10], Iter [200/2905] Loss: 5.5322
Epoch [10/10], Iter [300/2905] Loss: 5.5969
Epoch [10/10], Iter [400/2905] Loss: 5.5982
Epoch [10/10], Iter [500/2905] Loss: 5.5323
Epoch [10/10], Iter [600/2905] Loss: 5.4720
Epoch [10/10], Iter [700/2905] Loss: 5.4134
Epoch [10/10], Iter [800/2905] Loss: 5.4306
Epoch [10/10], Iter [900/2905] Loss: 5.3328
Epoch [10/10], Iter [1000/2905] Loss: 5.4060
Epoch [10/10], Iter [1100/2905] Loss: 5.4669
Epoch [10/10], Iter [1200/2905] Loss: 5.4880
Epoch [10/10], Iter [1300/2905] Loss: 5.4193
Epoch [10/10], Iter [1400/2905] Loss: 5.4522
Epoch [10/10], Iter [1500/2905] Loss: 5.4573
Epoch [10/10], Iter [1600/2905] Loss: 5.3740
Epoch [10/10], Iter [1700/2905] Loss: 5.3935
Epoch [10/10], Iter [1800/2905] Loss: 5.4047
Epoch [10/10], Iter [1900/2905] Loss: 5.4407
Epoch [10/10], Iter [2000/2905] Loss: 5.3537
Epoch [10/10], Iter [2100/2905] Loss: 5.4439
Epoch [10/10], Iter [2200/2905] Loss: 5.4322
Epoch [10/10], Iter [2300/2905] Loss: 5.3861
Epoch [10/10], Iter [2400/2905] Loss: 5.4295
Epoch [10/10], Iter [2500/2905] Loss: 5.4911
Epoch [10/10], Iter [2600/2905] Loss: 5.4275
Epoch [10/10], Iter [2700/2905] Loss: 5.3235
Epoch [10/10], Iter [2800/2905] Loss: 5.3341
Epoch [10/10], Iter [2900/2905] Loss: 5.2714
Val acc, prec, ppl 0.0004761904761904762 0.0073042758218782805 122318.328125
Traceback (most recent call last):
  File "lstm.py", line 208, in <module>
    predwords = [TEXT.vocab.itos[x] for x in predicted]
  File "lstm.py", line 208, in <listcomp>
    predwords = [TEXT.vocab.itos[x] for x in predicted]
TypeError: list indices must be integers or slices, not list
[01;32melbertgong@nlpfinal[00m:[01;34m/mnt/trunk/cs287-s18/HW2[00m$ exit
exit
