[01;32melbertgong@nlpfinal[00m:[01;34m/mnt/trunk/cs287-s18/HW2[00m$ python3 lstm.py --[Km 'ml[K[K.././[K./models/HW2/lstm3.pkl'
len(train) 1
len(TEXT.vocab) 10001
Size of text batch [max bptt length, batch size] torch.Size([32, 10])
Second in batch Variable containing:
    8
  202
   77
    5
  183
  561
 3837
   18
  975
  976
    7
  943
    5
  157
   78
 1571
  289
  645
    3
   30
  132
    0
   20
    2
  273
 7821
   17
    9
  117
 2815
  969
    6
[torch.LongTensor of size 32]

Converted back to string:  in part because of buy programs generated by stock-index arbitrage a form of program trading involving futures contracts <eos> but interest <unk> as the day wore on and investors looked ahead to
Converted back to string:  the release later this week of two important economic reports <eos> the first is wednesday 's survey of purchasing managers considered a good indicator of how the nation 's manufacturing sector fared
Word embeddings size  torch.Size([10001, 300])
REMINDER!!! Did you create ../../models/HW2?????
CUDA is available, assigning to GPU.
Epoch [1/5], Iter [100/2905] Loss: 215.6296
Epoch [1/5], Iter [200/2905] Loss: 211.0600
Epoch [1/5], Iter [300/2905] Loss: 208.2957
Epoch [1/5], Iter [400/2905] Loss: 205.6217
Epoch [1/5], Iter [500/2905] Loss: 203.7243
Epoch [1/5], Iter [600/2905] Loss: 198.9927
Epoch [1/5], Iter [700/2905] Loss: 196.0715
Epoch [1/5], Iter [800/2905] Loss: 192.8805
Epoch [1/5], Iter [900/2905] Loss: 189.8220
Epoch [1/5], Iter [1000/2905] Loss: 187.6366
Epoch [1/5], Iter [1100/2905] Loss: 185.7702
Epoch [1/5], Iter [1200/2905] Loss: 183.3925
Epoch [1/5], Iter [1300/2905] Loss: 181.8928
Epoch [1/5], Iter [1400/2905] Loss: 180.4027
Epoch [1/5], Iter [1500/2905] Loss: 179.1524
Epoch [1/5], Iter [1600/2905] Loss: 176.4999
^[Epoch [1/5], Iter [1700/2905] Loss: 176.1115
Epoch [1/5], Iter [1800/2905] Loss: 175.0112
Epoch [1/5], Iter [1900/2905] Loss: 174.8322
[01;32melbertgong@nlpfinal[00m:[01;34m/mnt/trunk/cs287-s18/HW2[00m$ python3 nnlm.py m[K-m Epoch [1/5], Iter [2000/2905] Loss: 173.3689
Epoch [1/5], Iter [2100/2905] Loss: 174.0751
Epoch [1/5], Iter [2200/2905] Loss: 172.3815
'm[K../../models/nnlm.Epoch [1/5], Iter [2300/2905] Loss: 170.9527
[K2.[K[K3.pkl'
len(train) 1
len(TEXT.vocab) 10001
Size of text batch [max bptt length, batch size] torch.Size([32, 10])
Second in batch Variable containing:
    8
  202
   77
    5
  183
  561
 3837
   18
  975
  976
    7
  943
    5
  157
   78
 1571
  289
  645
    3
   30
  132
    0
   20
    2
  273
 7821
   17
    9
  117
 2815
  969
    6
[torch.LongTensor of size 32]

Converted back to string:  in part because of buy programs generated by stock-index arbitrage a form of program trading involving futures contracts <eos> but interest <unk> as the day wore on and investors looked ahead to
Converted back to string:  the release later this week of two important economic reports <eos> the first is wednesday 's survey of purchasing managers considered a good indicator of how the nation 's manufacturing sector fared
Word embeddings size  torch.Size([10001, 300])
REMINDER!!! Did you create ../../models/HW2?????
Epoch [1/5], Iter [2400/2905] Loss: 169.8697
Epoch [1/5], Iter [2500/2905] Loss: 169.1198
Epoch [1/5], Iter [2600/2905] Loss: 167.4011
Epoch [1/5], Iter [2700/2905] Loss: 166.3462
Epoch [1/5], Iter [2800/2905] Loss: 165.6443
Epoch [1/5], Iter [2900/2905] Loss: 165.1128
Epoch [1/10], Iter [100/2905] Loss: 5.0483
Val acc, prec, ppl 0.19449888017917133 1.1840783011124438 205.52804565429688
Epoch [2/5], Iter [100/2905] Loss: 164.2780
Epoch [2/5], Iter [200/2905] Loss: 163.6676
Epoch [2/5], Iter [300/2905] Loss: 162.7874
Epoch [1/10], Iter [200/2905] Loss: 6.0805
Epoch [2/5], Iter [400/2905] Loss: 161.9697
Epoch [2/5], Iter [500/2905] Loss: 160.9796
Epoch [2/5], Iter [600/2905] Loss: 160.1778
Epoch [2/5], Iter [700/2905] Loss: 160.2315
Epoch [2/5], Iter [800/2905] Loss: 159.8218
Epoch [2/5], Iter [900/2905] Loss: 158.8642
Epoch [1/10], Iter [300/2905] Loss: 5.6738
Epoch [2/5], Iter [1000/2905] Loss: 158.7820
Epoch [2/5], Iter [1100/2905] Loss: 158.5298
Epoch [2/5], Iter [1200/2905] Loss: 157.5295
Epoch [2/5], Iter [1300/2905] Loss: 157.1784
Epoch [2/5], Iter [1400/2905] Loss: 156.8487
Epoch [2/5], Iter [1500/2905] Loss: 156.7039
Epoch [1/10], Iter [400/2905] Loss: 6.6888
Epoch [2/5], Iter [1600/2905] Loss: 154.5968
Epoch [2/5], Iter [1700/2905] Loss: 155.1952
Epoch [2/5], Iter [1800/2905] Loss: 154.7120
Epoch [2/5], Iter [1900/2905] Loss: 155.2117
Epoch [2/5], Iter [2000/2905] Loss: 153.9308
Epoch [2/5], Iter [2100/2905] Loss: 155.6024
Epoch [1/10], Iter [500/2905] Loss: 5.6846
Epoch [2/5], Iter [2200/2905] Loss: 154.1600
Epoch [2/5], Iter [2300/2905] Loss: 153.2671
Epoch [2/5], Iter [2400/2905] Loss: 152.8342
Epoch [2/5], Iter [2500/2905] Loss: 152.7012
Epoch [2/5], Iter [2600/2905] Loss: 151.3916
Epoch [2/5], Iter [2700/2905] Loss: 150.6307
Epoch [1/10], Iter [600/2905] Loss: 4.1034
Epoch [2/5], Iter [2800/2905] Loss: 150.5555
Epoch [2/5], Iter [2900/2905] Loss: 150.2355
Val acc, prec, ppl 0.2187010078387458 1.2918280346652813 156.60894775390625
Epoch [1/10], Iter [700/2905] Loss: 5.5312
Epoch [3/5], Iter [100/2905] Loss: 149.4416
Epoch [3/5], Iter [200/2905] Loss: 148.8112
Epoch [3/5], Iter [300/2905] Loss: 148.4513
Epoch [3/5], Iter [400/2905] Loss: 147.6006
Epoch [3/5], Iter [500/2905] Loss: 146.6728
Epoch [3/5], Iter [600/2905] Loss: 146.1323
Epoch [1/10], Iter [800/2905] Loss: 5.9053
Epoch [3/5], Iter [700/2905] Loss: 146.3978
Epoch [3/5], Iter [800/2905] Loss: 146.1272
Epoch [3/5], Iter [900/2905] Loss: 145.2880
Epoch [3/5], Iter [1000/2905] Loss: 145.7396
Epoch [3/5], Iter [1100/2905] Loss: 145.9775
Epoch [3/5], Iter [1200/2905] Loss: 145.3140
Epoch [1/10], Iter [900/2905] Loss: 6.1210
Epoch [3/5], Iter [1300/2905] Loss: 144.9035
Epoch [3/5], Iter [1400/2905] Loss: 145.0716
Epoch [3/5], Iter [1500/2905] Loss: 145.0403
Epoch [3/5], Iter [1600/2905] Loss: 143.0512
Epoch [3/5], Iter [1700/2905] Loss: 144.0823
Epoch [3/5], Iter [1800/2905] Loss: 144.0152
Epoch [3/5], Iter [1900/2905] Loss: 144.5754
Epoch [1/10], Iter [1000/2905] Loss: 6.1960
Epoch [3/5], Iter [2000/2905] Loss: 143.2541
Epoch [3/5], Iter [2100/2905] Loss: 145.0868
Epoch [3/5], Iter [2200/2905] Loss: 143.6222
Epoch [3/5], Iter [2300/2905] Loss: 142.9940
Epoch [3/5], Iter [2400/2905] Loss: 142.8457
Epoch [3/5], Iter [2500/2905] Loss: 143.0818
Epoch [1/10], Iter [1100/2905] Loss: 5.4170
Epoch [3/5], Iter [2600/2905] Loss: 141.8405
Epoch [3/5], Iter [2700/2905] Loss: 140.9769
Epoch [3/5], Iter [2800/2905] Loss: 140.9056
Epoch [3/5], Iter [2900/2905] Loss: 140.5036
Epoch [1/10], Iter [1200/2905] Loss: 5.1807
Val acc, prec, ppl 0.23013717805151176 1.3427584808611823 143.27085876464844
Epoch [4/5], Iter [100/2905] Loss: 139.6135
Epoch [4/5], Iter [200/2905] Loss: 139.1136
Epoch [4/5], Iter [300/2905] Loss: 138.9695
Epoch [4/5], Iter [400/2905] Loss: 138.0754
Epoch [1/10], Iter [1300/2905] Loss: 4.5620
Epoch [4/5], Iter [500/2905] Loss: 137.2461
Epoch [4/5], Iter [600/2905] Loss: 136.8489
Epoch [4/5], Iter [700/2905] Loss: 136.9471
Epoch [4/5], Iter [800/2905] Loss: 136.8049
Epoch [4/5], Iter [900/2905] Loss: 136.0044
Epoch [4/5], Iter [1000/2905] Loss: 136.5965
Epoch [4/5], Iter [1100/2905] Loss: 137.0812
Epoch [1/10], Iter [1400/2905] Loss: 8.4763
Epoch [4/5], Iter [1200/2905] Loss: 136.7513
Epoch [4/5], Iter [1300/2905] Loss: 136.2224
Epoch [4/5], Iter [1400/2905] Loss: 136.5935
Epoch [4/5], Iter [1500/2905] Loss: 136.6010
Epoch [4/5], Iter [1600/2905] Loss: 134.6896
Epoch [4/5], Iter [1700/2905] Loss: 135.7769
Epoch [1/10], Iter [1500/2905] Loss: 5.2600
Epoch [4/5], Iter [1800/2905] Loss: 136.0144
Epoch [4/5], Iter [1900/2905] Loss: 136.7054
Epoch [4/5], Iter [2000/2905] Loss: 135.4678
Epoch [4/5], Iter [2100/2905] Loss: 137.3032
Epoch [4/5], Iter [2200/2905] Loss: 136.0188
Epoch [4/5], Iter [2300/2905] Loss: 135.4106
Epoch [1/10], Iter [1600/2905] Loss: 6.5742
Epoch [4/5], Iter [2400/2905] Loss: 135.3657
Epoch [4/5], Iter [2500/2905] Loss: 135.7370
Epoch [4/5], Iter [2600/2905] Loss: 134.6622
Epoch [4/5], Iter [2700/2905] Loss: 133.5375
Epoch [4/5], Iter [2800/2905] Loss: 133.4307
Epoch [4/5], Iter [2900/2905] Loss: 132.9740
Epoch [1/10], Iter [1700/2905] Loss: 6.4416
Val acc, prec, ppl 0.23584826427771557 1.3656475476693248 142.9728240966797
Epoch [5/5], Iter [100/2905] Loss: 132.0307
Epoch [5/5], Iter [200/2905] Loss: 131.3499
Epoch [1/10], Iter [1800/2905] Loss: 5.2564
Epoch [5/5], Iter [300/2905] Loss: 131.4420
Epoch [5/5], Iter [400/2905] Loss: 130.7338
Epoch [5/5], Iter [500/2905] Loss: 130.0780
Epoch [5/5], Iter [600/2905] Loss: 129.6514
Epoch [5/5], Iter [700/2905] Loss: 129.8748
Epoch [5/5], Iter [800/2905] Loss: 129.9215
Epoch [1/10], Iter [1900/2905] Loss: 5.8947
Epoch [5/5], Iter [900/2905] Loss: 129.2448
Epoch [5/5], Iter [1000/2905] Loss: 129.8505
Epoch [5/5], Iter [1100/2905] Loss: 130.6445
Epoch [5/5], Iter [1200/2905] Loss: 130.4509
Epoch [5/5], Iter [1300/2905] Loss: 129.9863
Epoch [5/5], Iter [1400/2905] Loss: 130.3716
Epoch [5/5], Iter [1500/2905] Loss: 130.4229
Epoch [1/10], Iter [2000/2905] Loss: 6.8707
Epoch [5/5], Iter [1600/2905] Loss: 128.5270
Epoch [5/5], Iter [1700/2905] Loss: 129.6806
Epoch [5/5], Iter [1800/2905] Loss: 129.8783
Epoch [5/5], Iter [1900/2905] Loss: 130.5510
Epoch [5/5], Iter [2000/2905] Loss: 129.6180
Epoch [5/5], Iter [2100/2905] Loss: 131.4946
Epoch [1/10], Iter [2100/2905] Loss: 5.8132
Epoch [5/5], Iter [2200/2905] Loss: 130.1863
Epoch [5/5], Iter [2300/2905] Loss: 129.6663
Epoch [5/5], Iter [2400/2905] Loss: 129.7288
Epoch [5/5], Iter [2500/2905] Loss: 129.9611
Epoch [5/5], Iter [2600/2905] Loss: 128.9703
Epoch [5/5], Iter [2700/2905] Loss: 127.8334
Epoch [1/10], Iter [2200/2905] Loss: 8.5595
Epoch [5/5], Iter [2800/2905] Loss: 127.7558
Epoch [5/5], Iter [2900/2905] Loss: 127.3041
Val acc, prec, ppl 0.2394316909294513 1.3780219877791597 148.1148223876953
Epoch [1/10], Iter [2300/2905] Loss: 7.5995
Epoch [1/10], Iter [2400/2905] Loss: 5.7078
[01;32melbertgong@nlpfinal[00m:[01;34m/mnt/trunk/cs287-s18/HW2[00m$ Epoch [1/10], Iter [2500/2905] Loss: 6.6102
Epoch [1/10], Iter [2600/2905] Loss: 4.9816
Epoch [1/10], Iter [2700/2905] Loss: 5.5097
Epoch [1/10], Iter [2800/2905] Loss: 4.7961
Epoch [1/10], Iter [2900/2905] Loss: 4.3492
Val acc, prec, ppl 0.19064308681672026 1.1912872801259926 355.1040344238281
Epoch [2/10], Iter [100/2905] Loss: 4.1188
Epoch [2/10], Iter [200/2905] Loss: 6.0813
Epoch [2/10], Iter [300/2905] Loss: 4.8864
Epoch [2/10], Iter [400/2905] Loss: 6.0587
Epoch [2/10], Iter [500/2905] Loss: 4.9165
Epoch [2/10], Iter [600/2905] Loss: 2.5976
Epoch [2/10], Iter [700/2905] Loss: 5.6445
Epoch [2/10], Iter [800/2905] Loss: 5.2540
Epoch [2/10], Iter [900/2905] Loss: 5.5864
Epoch [2/10], Iter [1000/2905] Loss: 4.3443
Epoch [2/10], Iter [1100/2905] Loss: 4.8765
Epoch [2/10], Iter [1200/2905] Loss: 4.1617
Epoch [2/10], Iter [1300/2905] Loss: 3.9152
Epoch [2/10], Iter [1400/2905] Loss: 7.2226
Epoch [2/10], Iter [1500/2905] Loss: 4.6578
Epoch [2/10], Iter [1600/2905] Loss: 5.7712
Epoch [2/10], Iter [1700/2905] Loss: 4.8891
Epoch [2/10], Iter [1800/2905] Loss: 4.6207
Epoch [2/10], Iter [1900/2905] Loss: 5.7774
Epoch [2/10], Iter [2000/2905] Loss: 5.4631
Epoch [2/10], Iter [2100/2905] Loss: 4.6894
Epoch [2/10], Iter [2200/2905] Loss: 7.6458
Epoch [2/10], Iter [2300/2905] Loss: 6.4488
Epoch [2/10], Iter [2400/2905] Loss: 4.5875
Epoch [2/10], Iter [2500/2905] Loss: 4.8453
Epoch [2/10], Iter [2600/2905] Loss: 4.9864
Epoch [2/10], Iter [2700/2905] Loss: 4.5319
Epoch [2/10], Iter [2800/2905] Loss: 4.8839
Epoch [2/10], Iter [2900/2905] Loss: 3.9291
Val acc, prec, ppl 0.17983922829581994 1.1397720469842243 479.1932067871094
Epoch [3/10], Iter [100/2905] Loss: 3.4893
Epoch [3/10], Iter [200/2905] Loss: 5.2445
Epoch [3/10], Iter [300/2905] Loss: 4.3093
xit[K[K[Kexit
exit
Epoch [3/10], Iter [400/2905] Loss: 5.3937
Epoch [3/10], Iter [500/2905] Loss: 4.5425
Epoch [3/10], Iter [600/2905] Loss: 2.0755
Epoch [3/10], Iter [700/2905] Loss: 5.3894
Epoch [3/10], Iter [800/2905] Loss: 4.3851
