{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: PyTorch Tutorial (Tensors, Automatic Differentation, Linear Regression, MNIST Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Goal\n",
    "\n",
    "1. Learn to work with/manipulate tensors in PyTorch\n",
    "2. Learn about automatic differentiation\n",
    "3. Fit a linear regression model\n",
    "3. Build a simple classifier on MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors, or multidimensional arrays, are fundamental objects that you will be working with in Torch (and deep learning in general). Let's create some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-8.8217e+21  4.5824e-41 -8.8217e+21  4.5824e-41  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "[torch.FloatTensor of size 5x5]\n",
      "\n",
      "\n",
      " 1.4045e+14  1.4045e+14\n",
      " 1.3967e+14  1.4045e+14\n",
      " 0.0000e+00  4.2950e+09\n",
      " 0.0000e+00  3.3000e+01\n",
      " 1.4045e+14  1.4045e+14\n",
      "[torch.LongTensor of size 5x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(5,5) #create a 5 x 5 tensor\n",
    "print(x)\n",
    "x = torch.LongTensor(5,2) #often our input data consists of integers (word indices), so it's helpful to use LongTensors\n",
    "print(x)\n",
    "#we can also do other initializations\n",
    "x = torch.randn(3,4) #initialize from standard normal\n",
    "x = torch.ones(3,4) #all ones\n",
    "x = torch.zeros(3,4) #all zeros\n",
    "x = torch.eye(3) # identity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can go back between numpy/torch tensors with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.41250177 -1.70226126  0.24251604  0.6503295  -0.10149816]\n",
      "\n",
      "-0.4125\n",
      "-1.7023\n",
      " 0.2425\n",
      " 0.6503\n",
      "-0.1015\n",
      "[torch.DoubleTensor of size 5]\n",
      "\n",
      "[-0.41250177 -1.70226126  0.24251604  0.6503295  -0.10149816]\n"
     ]
    }
   ],
   "source": [
    "x_numpy = np.random.randn(5)\n",
    "print(x_numpy)\n",
    "x_torch = torch.from_numpy(x_numpy)\n",
    "print(x_torch)\n",
    "x_numpy2 = x_torch.numpy()\n",
    "print(x_numpy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing torch tensors is essentially identitical to the case in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3890511095523834\n",
      "\n",
      " 0.3891\n",
      "-0.2427\n",
      " 0.9648\n",
      "-0.2949\n",
      "[torch.FloatTensor of size 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4,3)\n",
    "print(x[0, 0]) \n",
    "print(x[:, 0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various ways to manipulate tensors. Of particular utility is the *view* function, which reshapes a tensor in memory. See [here](http://pytorch.org/docs/master/tensors.html) for more operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.3891 -1.0726 -1.4240 -0.2427 -0.0909  0.2823\n",
      " 0.9648  0.4201  0.0755 -0.2949 -0.4512  0.9135\n",
      "[torch.FloatTensor of size 2x6]\n",
      "\n",
      "\n",
      " 0.3891\n",
      "-1.0726\n",
      "-1.4240\n",
      "-0.2427\n",
      "-0.0909\n",
      " 0.2823\n",
      " 0.9648\n",
      " 0.4201\n",
      " 0.0755\n",
      "-0.2949\n",
      "-0.4512\n",
      " 0.9135\n",
      "[torch.FloatTensor of size 12]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x.view(2,6)) #reshape the 4x3 tensor into a 2x6 tensor\n",
    "print(x.view(-1)) # -1 always reshapes to a 1d tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use operations on tensors as in numpy. Operations that have an underscore _ are *in-place* and modify the original tensor. Other operations will create a new tensor in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-1.0718  0.5840 -0.1251 -0.1273 -0.3673\n",
      "-0.4221 -0.3984 -0.2768  1.2538 -1.7938\n",
      "-0.0235 -0.6884 -0.7418  0.9392  0.0808\n",
      "-0.5981  0.4892  0.4142 -1.2865 -1.2224\n",
      "-2.1484 -0.5774 -1.5540  0.8833  0.6478\n",
      "[torch.FloatTensor of size 5x5]\n",
      "\n",
      "\n",
      "-1.0718  0.5840 -0.1251 -0.1273 -0.3673\n",
      "-0.4221 -0.3984 -0.2768  1.2538 -1.7938\n",
      "-0.0235 -0.6884 -0.7418  0.9392  0.0808\n",
      "-0.5981  0.4892  0.4142 -1.2865 -1.2224\n",
      "-2.1484 -0.5774 -1.5540  0.8833  0.6478\n",
      "[torch.FloatTensor of size 5x5]\n",
      "\n",
      "\n",
      " 0.1727  0.1862  1.4901 -0.2699 -0.1862\n",
      "-0.4406 -1.3094 -0.3104  0.8161 -0.5163\n",
      " 1.2688  0.2149 -1.3392 -0.9872  1.6290\n",
      " 0.3489  0.8104  0.3664 -1.2595 -3.0184\n",
      "-2.7445 -0.1004 -1.9155 -0.0486  0.9707\n",
      "[torch.FloatTensor of size 5x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 5)\n",
    "y = torch.randn(5, 5)\n",
    "z = torch.mm(x, y) #matrix multiply\n",
    "z = 2*x # scalar multiplication\n",
    "z = x + y #addition, creates a new tensor\n",
    "print(x)\n",
    "z = x.add(y) #same\n",
    "print(x)\n",
    "x.add_(y) #modifies x by adding y to it\n",
    "print(x)\n",
    "#there are other operations such as x.mul_(), x.div_() etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, so the tensor object is basically the same as a numpy array. What's neat is that you can define **computation graphs** with tensors and backpropagate gradients through this computational graph automatically. This is known as automatic differentation. To do this however, we need to use a `Variable` object which is basically a wrapper around a tensor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.9488\n",
      "-0.4401\n",
      "-0.3082\n",
      "-0.4166\n",
      " 0.1723\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "x = Variable(torch.randn(5), requires_grad=True) #requires grad means that we want to calculate gradients\n",
    "print(x.data) #a tensor object\n",
    "print(x.grad) #the gradient. right now we haven't calculated any gradients so there is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a simple computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.3710\n",
      "-1.0819\n",
      " 1.1684\n",
      "-1.7782\n",
      " 4.4008\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "Variable containing:\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.randn(5), requires_grad=True) \n",
    "y = x.mul(2) #y is now a variable. almost any operation you can do on tensors you can do on Variables\n",
    "print(y)\n",
    "y.backward(Variable(torch.ones(y.size()))) #this is a tricky concept, but we essentially backprop a vector of ones\n",
    "#to simulate the calculation of dy[i] / dx for all i\n",
    "print(x.grad) #this should be a vector of 2s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in practice, we almost always calculate the derivatve with respect to a **scalar**. In this case we can simply call `.backward()` without having to backpropagate a 1x1 vector of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x.grad.data.zero_() #gradients are always accumulated, so we need to zero them out manually\n",
    "y = x.mul(2).sum() # now y is a scalar. In most cases this would be your average loss\n",
    "y.backward() #this is equivalent to y.backward(Variable(torch.ones(y.size())))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit a simple least squares model on a synthetic dataset with gradient descent. First let's generate synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_points = 1000\n",
    "num_features = 5\n",
    "w_star = torch.randn(num_features) #true weight vector\n",
    "x = torch.randn(num_points, num_features) #input data\n",
    "y = torch.mm(x, w_star.view(5, 1)) #torch.mm expects both inputs to be matrices so we cast w_star to be a column vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem has an analytic solution which we can calculate directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-0.5498\n",
      "-0.8299\n",
      "-0.4888\n",
      " 2.0011\n",
      " 0.1236\n",
      "[torch.FloatTensor of size 5x1]\n",
      "\n",
      "\n",
      "-0.5498\n",
      "-0.8299\n",
      "-0.4888\n",
      " 2.0011\n",
      " 0.1236\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w_ols = torch.mm(torch.mm(torch.inverse(torch.mm(x.t(), x)), x.t()), y) #(X^T X)^{-1} X^T Y\n",
    "print(w_ols)\n",
    "print(w_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's see if we can obtain (approximately) the same solution with gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.9462640285491943\n",
      "10 0.02835780568420887\n",
      "20 0.00031745442538522184\n",
      "30 3.945892331103096e-06\n",
      "40 5.220245213877206e-08\n",
      "Variable containing:\n",
      "-0.5498\n",
      "-0.8299\n",
      "-0.4888\n",
      " 2.0011\n",
      " 0.1236\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "\n",
      "-0.5498\n",
      "-0.8299\n",
      "-0.4888\n",
      " 2.0011\n",
      " 0.1236\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w_sgd = Variable(torch.randn(num_features), requires_grad=True) #randomly initialize\n",
    "x_sgd = Variable(x) #remember, we need to convert everything to Variables to work with automatic differentiation\n",
    "y_sgd = Variable(y) #we don't need to calculate gradients with respect to these guys\n",
    "\n",
    "num_iters = 50\n",
    "learning_rate = 0.1\n",
    "\n",
    "for i in range(num_iters):\n",
    "    if w_sgd.grad is not None:\n",
    "        w_sgd.grad.zero_() #gradients get accumulated so we have to manually zero them out\n",
    "    y_pred = torch.mm(x_sgd, w_sgd.unsqueeze(1)) #unsqueeze adds an extra dimension\n",
    "    error = (y_sgd - y_pred)**2\n",
    "    error_avg = error.mean()\n",
    "    if i % 10 == 0:\n",
    "        print(i, error_avg.data[0])\n",
    "    error_avg.backward()    \n",
    "    w_sgd.data = w_sgd.data - learning_rate*w_sgd.grad.data\n",
    "        \n",
    "print(w_sgd)\n",
    "print(w_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that worked, but it's a bit annoying to separately define the weights as `Variables` and manually apply matrix-multiplies. Fortunately, torch provides an `nn` package which provides abstractions for almost all of the layers that we will use in the course. Let's see a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "-0.4037  0.3182 -0.1677  0.3019  0.4190\n",
      "[torch.FloatTensor of size 1x5]\n",
      "\n",
      "Variable containing:\n",
      " 5.3384e-01\n",
      "-2.1461e+00\n",
      "-4.5379e-02\n",
      "     ⋮      \n",
      " 1.4982e-01\n",
      " 2.2410e-01\n",
      "-6.0813e-01\n",
      "[torch.FloatTensor of size 1000x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "lin = nn.Linear(num_features, 1, bias=False) #this defines a linear layer that goes from num_feature dimensions to 1\n",
    "print(lin.weight) #weight parameter of the linear layer. if bias = True, then we can access bias with lin.bias\n",
    "y_pred = lin(x_sgd) #lin(x_sgd) automatically calls forward on the input x_sgd\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of `nn` layers can be found [here](http://pytorch.org/docs/master/nn.html). You should try to familiarize yourself with pretty much all the `nn` layers. Torch also provides an `optim` package that makes parameter updates easier. We will be working with SGD in this tutorial, but other optimization algorithms (Adam, Adagrad, RMSProp, etc) can be found [here](http://pytorch.org/docs/master/optim.html). Let's try to fit the linear regression model with these abstractions now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "-0.5498 -0.8299 -0.4888  2.0011  0.1236\n",
      "[torch.FloatTensor of size 1x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(lin.parameters(), lr = learning_rate) #initialize with parameters\n",
    "for i in range(num_iters):\n",
    "    optimizer.zero_grad() #this will zero out the gradients of all your parameters\n",
    "    y_pred = lin(x_sgd)\n",
    "    error = (y_sgd - y_pred)**2\n",
    "    error_avg = error.mean()    \n",
    "    error_avg.backward()    \n",
    "    optimizer.step()    \n",
    "print(lin.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "val_dataset = datasets.MNIST(root='./data/',\n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "image, label = train_dataset[0]\n",
    "print(image.size())\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch provides a loader around tensor datasets so you can create/access mini-batches easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "for (image, label) in train_loader: #iterates through the dataset in mini-batches\n",
    "    print(image.size())\n",
    "    print(label.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we are ready to create our first model, which will be a simple multilayer perceptron. To do this, it helps to define a `class` with all the layers inside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (hidden_to_output): Linear(in_features=10, out_features=10)\n",
      "  (hidden_layers): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=10)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (logsoftmax): LogSoftmax()\n",
      ")\n",
      "torch.Size([10, 10])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 784])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_layers = 1, input_dim = 28*28, output_dim = 10, hidden_dim = 10):\n",
    "        super(MLP, self).__init__()                \n",
    "        self.hidden_to_output = nn.Linear(hidden_dim, output_dim)\n",
    "        hidden_layers = []\n",
    "        for l in range(num_layers):\n",
    "            dim = input_dim if l == 0 else hidden_dim #first layer is input to hidden layer\n",
    "            hidden_layers.append(nn.Linear(dim, hidden_dim))\n",
    "            hidden_layers.append(nn.ReLU()) #let's work with relu nonlinearities for now\n",
    "        self.hidden_layers = nn.Sequential(*hidden_layers) #Sequential module will apply the layers in sequence\n",
    "        self.logsoftmax = nn.LogSoftmax() #softmax will turn the output into probabilities, but log is more convnient\n",
    "        \n",
    "    def forward(self, x): #MLP(x) is shorthand for MLP.forward(x)\n",
    "        x_flatten = x.view(x.size(0), -1) #need to flatten batch_size x 1 x 28 x 28 to batch_size x 28*28\n",
    "        out = self.hidden_layers(x_flatten)\n",
    "        out = self.hidden_to_output(out) #you can redefine variables\n",
    "        return self.logsoftmax(out)\n",
    "        \n",
    "mlp = MLP(num_layers = 1)\n",
    "print(mlp)\n",
    "for p in mlp.parameters(): #all the parameters that were defined inside the module can be accessed like this\n",
    "    print(p.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the network is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-2.1279 -2.1694 -2.5179  ...  -2.4387 -2.2534 -2.6574\n",
      "-2.0849 -2.1479 -2.4743  ...  -2.5313 -2.1918 -2.7629\n",
      "-2.1240 -2.1286 -2.5494  ...  -2.5294 -2.2164 -2.6847\n",
      "          ...             ⋱             ...          \n",
      "-2.0931 -2.1249 -2.4294  ...  -2.5304 -2.1925 -2.7366\n",
      "-2.1732 -2.1386 -2.4708  ...  -2.4487 -2.2507 -2.6737\n",
      "-2.1267 -2.1063 -2.5193  ...  -2.5290 -2.2171 -2.6875\n",
      "[torch.FloatTensor of size 100x10]\n",
      "\n",
      "Variable containing:\n",
      " 0.1191\n",
      " 0.1142\n",
      " 0.0806\n",
      " 0.0965\n",
      " 0.1014\n",
      " 0.1160\n",
      " 0.1096\n",
      " 0.0873\n",
      " 0.1050\n",
      " 0.0701\n",
      "[torch.FloatTensor of size 10]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoonkim/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "y_pred = mlp(Variable(image))\n",
    "print(y_pred) #these will be log probabilities over each of the 10 classes\n",
    "print(y_pred[0].exp()) #let's make sure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also convenient to define a test function that we can call periodically to check performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoonkim/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation performance. NLL: 2.3145, Accuracy: 0.1506\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.NLLLoss() #this is the negative log-likelihood for multi-class classification\n",
    "\n",
    "def test(model, data):\n",
    "    correct = 0.\n",
    "    num_examples = 0.\n",
    "    nll = 0.\n",
    "    for (image, label) in data:\n",
    "        image, label = Variable(image), Variable(label) #annoying, but necessary        \n",
    "        y_pred = mlp(image)\n",
    "        nll_batch = criterion(y_pred, label)\n",
    "        nll += nll_batch.data[0] * image.size(0) #by default NLL is averaged over each batch\n",
    "        y_pred_max, y_pred_argmax = torch.max(y_pred, 1) #prediction is the argmax\n",
    "        correct += (y_pred_argmax.data == label.data).sum() \n",
    "        num_examples += image.size(0) \n",
    "    return nll/num_examples, correct/num_examples\n",
    "nll, accuracy = test(mlp, val_loader)\n",
    "print('Validation performance. NLL: %.4f, Accuracy: %.4f'% (nll, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(num_layers = 1)\n",
    "optim = torch.optim.SGD(mlp.parameters(), lr =0.5)\n",
    "num_epochs = 20\n",
    "for e in range(num_epochs):\n",
    "    for (image, label) in train_loader:\n",
    "        optim.zero_grad()\n",
    "        image, label = Variable(image), Variable(label) \n",
    "        y_pred = mlp(image)\n",
    "        nll_batch = criterion(y_pred, label)    \n",
    "        nll_batch.backward()\n",
    "        optim.step()\n",
    "    nll_train, accuracy_train = test(mlp, train_loader) #you never wanna do this in practice, since this will take forever\n",
    "    nll_val, accuracy_val = test(mlp, val_loader)\n",
    "    print('Training performance after epoch %d: NLL: %.4f, Accuracy: %.4f'% (e+1, nll_train, accuracy_train))\n",
    "    print('Validation performance after epoch %d: NLL: %.4f, Accuracy: %.4f'% (e+1, nll_val, accuracy_val))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with more hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(num_layers = 1, hidden_dim = 100)\n",
    "print(mlp)\n",
    "optim = torch.optim.SGD(mlp.parameters(), lr = 0.5)\n",
    "num_epochs = 20\n",
    "for e in range(num_epochs):\n",
    "    for (image, label) in train_loader:\n",
    "        optim.zero_grad()\n",
    "        image, label = Variable(image), Variable(label) \n",
    "        y_pred = mlp(image)\n",
    "        nll_batch = criterion(y_pred, label)    \n",
    "        nll_batch.backward()\n",
    "        optim.step()\n",
    "    nll_train, accuracy_train = test(mlp, train_loader) \n",
    "    nll_val, accuracy_val = test(mlp, val_loader)\n",
    "    print('Training performance after epoch %d: NLL: %.4f, Accuracy: %.4f'% (e+1, nll_train, accuracy_train))\n",
    "    print('Validation performance after epoch %d: NLL: %.4f, Accuracy: %.4f'% (e+1, nll_val, accuracy_val))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. For the rest of section, try implementing a ConvNet. You may need to use more sophisticated optimization algorithms (`optim.Adam(model.parameters(), lr = 0.001)` should work well enough for most models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
